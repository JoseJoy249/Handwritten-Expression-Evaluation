{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm \n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import preprocess\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.models import model_from_json\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Dropout\n",
    "\n",
    "import keras\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating dictionary of labels\n",
    "class_labels = {str(x):x for x in range(10)}\n",
    "class_labels.update({'+':10, 'times':11, '-':12 })\n",
    "label_class = dict( zip(class_labels.values(), class_labels.keys() ))\n",
    "\n",
    "# Loading data from .npy file and spliting into training and validation sets\n",
    "path = '/Users/josejoy/Desktop/ECE 271B Stat Learning /project/Training Data/'\n",
    "data, labels = preprocess.load_data(class_labels, path+'data_ver1.npy' , path+'labels_ver1.npy'\n",
    "                                   , train = 0.99 , val = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTcCEjJitWQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12d85f1d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label =  6\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.randint(data['train'].shape[0])\n",
    "plt.imshow(data['train'][idx,:].reshape(28,28),cmap = 'gray')\n",
    "plt.show()\n",
    "print('Label = ',label_class[ np.argmax(labels['train'][idx,:])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras (single stage model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "21497/21497 [==============================] - 4s 176us/step - loss: 1.3371 - acc: 0.7536\n",
      "Epoch 2/30\n",
      "21497/21497 [==============================] - 3s 119us/step - loss: 0.4450 - acc: 0.9116\n",
      "Epoch 3/30\n",
      "21497/21497 [==============================] - 3s 119us/step - loss: 0.2738 - acc: 0.9343\n",
      "Epoch 4/30\n",
      "21497/21497 [==============================] - 3s 121us/step - loss: 0.2041 - acc: 0.9479\n",
      "Epoch 5/30\n",
      "21497/21497 [==============================] - 3s 120us/step - loss: 0.1674 - acc: 0.9551\n",
      "Epoch 6/30\n",
      "21497/21497 [==============================] - 3s 120us/step - loss: 0.1385 - acc: 0.9619\n",
      "Epoch 7/30\n",
      "21497/21497 [==============================] - 3s 121us/step - loss: 0.1223 - acc: 0.9669\n",
      "Epoch 8/30\n",
      "21497/21497 [==============================] - 3s 122us/step - loss: 0.1095 - acc: 0.9711\n",
      "Epoch 9/30\n",
      "21497/21497 [==============================] - 3s 122us/step - loss: 0.1005 - acc: 0.9703\n",
      "Epoch 10/30\n",
      "21497/21497 [==============================] - 3s 123us/step - loss: 0.0937 - acc: 0.9734\n",
      "Epoch 11/30\n",
      "21497/21497 [==============================] - 3s 130us/step - loss: 0.0804 - acc: 0.9773\n",
      "Epoch 12/30\n",
      "21497/21497 [==============================] - 2s 115us/step - loss: 0.0758 - acc: 0.9779\n",
      "Epoch 13/30\n",
      "21497/21497 [==============================] - 3s 120us/step - loss: 0.0725 - acc: 0.9801\n",
      "Epoch 14/30\n",
      "21497/21497 [==============================] - 3s 121us/step - loss: 0.0616 - acc: 0.9816 1s - loss:\n",
      "Epoch 15/30\n",
      "21497/21497 [==============================] - 3s 120us/step - loss: 0.0568 - acc: 0.9840\n",
      "Epoch 16/30\n",
      "21497/21497 [==============================] - 3s 122us/step - loss: 0.0594 - acc: 0.9824\n",
      "Epoch 17/30\n",
      "21497/21497 [==============================] - 2s 114us/step - loss: 0.0505 - acc: 0.9850\n",
      "Epoch 18/30\n",
      "21497/21497 [==============================] - 2s 107us/step - loss: 0.0496 - acc: 0.9847\n",
      "Epoch 19/30\n",
      "21497/21497 [==============================] - 3s 123us/step - loss: 0.0460 - acc: 0.9865\n",
      "Epoch 20/30\n",
      "21497/21497 [==============================] - 3s 122us/step - loss: 0.0469 - acc: 0.9863\n",
      "Epoch 21/30\n",
      "21497/21497 [==============================] - 3s 122us/step - loss: 0.0422 - acc: 0.9874\n",
      "Epoch 22/30\n",
      "21497/21497 [==============================] - 3s 123us/step - loss: 0.0426 - acc: 0.9871\n",
      "Epoch 23/30\n",
      "21497/21497 [==============================] - 3s 126us/step - loss: 0.0384 - acc: 0.9890\n",
      "Epoch 24/30\n",
      "21497/21497 [==============================] - 3s 121us/step - loss: 0.0353 - acc: 0.9899\n",
      "Epoch 25/30\n",
      "21497/21497 [==============================] - 3s 123us/step - loss: 0.0351 - acc: 0.9891\n",
      "Epoch 26/30\n",
      "21497/21497 [==============================] - 3s 122us/step - loss: 0.0403 - acc: 0.9875\n",
      "Epoch 27/30\n",
      "21497/21497 [==============================] - 3s 123us/step - loss: 0.0354 - acc: 0.9887\n",
      "Epoch 28/30\n",
      "21497/21497 [==============================] - 3s 134us/step - loss: 0.0363 - acc: 0.9887\n",
      "Epoch 29/30\n",
      "21497/21497 [==============================] - 3s 139us/step - loss: 0.0286 - acc: 0.9912\n",
      "Epoch 30/30\n",
      "21497/21497 [==============================] - 3s 135us/step - loss: 0.0264 - acc: 0.9926\n",
      "217/217 [==============================] - 0s 2ms/step\n",
      "Model in testing  =  [0.041475638747215271, 0.98617511987686157]\n",
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "# Single stage model \n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(256, input_dim=784, kernel_initializer ='uniform'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.3))\n",
    "   \n",
    "model.add(Dense(128, kernel_initializer ='uniform'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Dense(64, kernel_initializer ='uniform'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Dense(13, kernel_initializer ='uniform', activation='softmax'))\n",
    "\n",
    "\n",
    "# model.add(Dense(256, input_dim = 784, kernel_initializer='uniform', activation='relu'))\n",
    "# model.add(Dense(128, kernel_initializer='uniform', activation='relu'))\n",
    "# model.add(Dense(64, kernel_initializer='uniform', activation='relu'))\n",
    "# model.add(Dense(16, kernel_initializer='uniform', activation='relu'))\n",
    "# model.add(Dense(13, kernel_initializer='uniform', activation='softmax'))\n",
    "\n",
    "adam1 = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999)\n",
    "model.compile(loss=keras.losses.categorical_crossentropy, optimizer=adam1,metrics=['accuracy'] )\n",
    "\n",
    "model.fit(data['train'], labels['train'], epochs=30, batch_size=256)\n",
    "eval1 = model.evaluate(data['val'], labels['val'], batch_size=256)\n",
    "print('Model in testing  = ',eval1)\n",
    "\n",
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"./trained models/MLP_singlestage_ver1.1.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"./trained models/MLP_singlestage_ver1.1.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras multistage model (Under construction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "74927/74927 [==============================] - 5s 67us/step - loss: 0.8968 - acc: 0.7292\n",
      "Epoch 2/10\n",
      "74927/74927 [==============================] - 4s 50us/step - loss: 0.3175 - acc: 0.9103\n",
      "Epoch 3/10\n",
      "74927/74927 [==============================] - 4s 60us/step - loss: 0.2258 - acc: 0.9365\n",
      "Epoch 4/10\n",
      "74927/74927 [==============================] - 5s 60us/step - loss: 0.1718 - acc: 0.9520\n",
      "Epoch 5/10\n",
      "74927/74927 [==============================] - 5s 60us/step - loss: 0.1390 - acc: 0.9610\n",
      "Epoch 6/10\n",
      "74927/74927 [==============================] - 4s 56us/step - loss: 0.1185 - acc: 0.9664\n",
      "Epoch 7/10\n",
      "74927/74927 [==============================] - 4s 58us/step - loss: 0.1027 - acc: 0.9712\n",
      "Epoch 8/10\n",
      "74927/74927 [==============================] - 4s 53us/step - loss: 0.0912 - acc: 0.9746\n",
      "Epoch 9/10\n",
      "74927/74927 [==============================] - 4s 57us/step - loss: 0.0799 - acc: 0.9777\n",
      "Epoch 10/10\n",
      "74927/74927 [==============================] - 4s 57us/step - loss: 0.0725 - acc: 0.9793\n",
      "8325/8325 [==============================] - 0s 52us/step\n",
      "Model in testing  =  [0.096790253115935365, 0.97153153191815622]\n",
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "label_full = {}\n",
    "temp1 = np.argmax(labels['train'],1).reshape(-1,1)\n",
    "temp1 = np.column_stack((temp1,temp1))\n",
    "temp1[temp1[:,0]<10,0] = 0\n",
    "temp1[temp1[:,0]>=10,0] = 1\n",
    "temp1[temp1[:,1]<10,1] = 1\n",
    "temp1[temp1[:,1]>=10,1] = 0\n",
    "label_full['train'] = temp1\n",
    "\n",
    "temp1 = np.argmax(labels['val'],1).reshape(-1,1)\n",
    "temp1 = np.column_stack((temp1,temp1))\n",
    "temp1[temp1[:,0]<10,0] = 0\n",
    "temp1[temp1[:,0]>=10,0] = 1\n",
    "temp1[temp1[:,1]<10,1] = 1\n",
    "temp1[temp1[:,1]>=10,1] = 0\n",
    "label_full['val'] = temp1\n",
    "\n",
    "stage1 = Sequential()\n",
    "stage1.add(Dense(64, input_dim = 784, kernel_initializer='uniform', activation='relu'))\n",
    "stage1.add(Dense(16, kernel_initializer='uniform', activation='relu'))\n",
    "stage1.add(Dense(2, kernel_initializer='uniform', activation='softmax'))\n",
    "\n",
    "adam1 = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999)\n",
    "stage1.compile(loss=keras.losses.categorical_crossentropy, optimizer=adam1,metrics=['accuracy'] )\n",
    "\n",
    "stage1.fit(data['train'], labels['train'], epochs=10, batch_size=256)\n",
    "eval1 = stage1.evaluate(data['val'], labels['val'], batch_size=256)\n",
    "print('Model in testing  = ',eval1)\n",
    "\n",
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"./trained models/MLP_singlestage_ver2.1.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"./trained models/MLP_singlestage_ver2.1.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow Models (Early stage; requires work; dont use )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 100\n",
    "batch_size = 100\n",
    "display_step = 1\n",
    "\n",
    "# Network Parameters\n",
    "n_hidden_1 = 256 # 1st layer number of neurons\n",
    "n_hidden_2 = 64 # 1st layer number of neurons\n",
    "n_input = 784 # MNIST data input (img shape: 28*28)\n",
    "n_classes = labels['val'].shape[1] # MNIST total classes (0-9 digits)\n",
    "\n",
    "# tf Graph input\n",
    "X = tf.placeholder(\"float\", [None, n_input], name = 'Input')\n",
    "Y = tf.placeholder(\"float\", [None, n_classes], name = 'Labels')\n",
    "drop = tf.placeholder(\"float\", name = 'droput_prob')\n",
    "\n",
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1]), name = 'weights1'),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2]), name = 'weights2'),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes]), name = 'weightsout')\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1]), name = 'bias1'),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2]), name = 'bias2'),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]), name = 'biasout')\n",
    "}\n",
    "\n",
    "# Create model\n",
    "def multilayer_perceptron(x):\n",
    "    # Hidden fully connected layer \n",
    "    layer_1 = tf.nn.dropout( tf.nn.relu(tf.add(tf.matmul(x, weights['h1']), biases['b1'])), drop)\n",
    "#     Hidden fully connected layer \n",
    "    layer_2 = tf.nn.dropout( tf.nn.relu(tf.add(tf.matmul(layer_1, weights['h2']), biases['b2']) ), drop)\n",
    "    # Hidden fully connected layer with 256 neurons\n",
    "    out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n",
    "    return out_layer\n",
    "\n",
    "# Construct model\n",
    "logits = multilayer_perceptron(X)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=logits, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Add ops to save and restore all the variables.\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    writer = tf.summary.FileWriter('logs', sess.graph)\n",
    "\n",
    "    # Training cycle\n",
    "    for epoch in tqdm(range(training_epochs)):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int( data['train'].shape[0]/batch_size)\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            if i < total_batch - 1:\n",
    "                batch_x =  data['train'][i*batch_size : (i+1)*batch_size, : ]\n",
    "                batch_y = labels['train'][i*batch_size : (i+1)*batch_size, : ]\n",
    "            else : \n",
    "                batch_x =  data['train'][i*batch_size : , : ]\n",
    "                batch_y = labels['train'][i*batch_size : , : ]\n",
    "\n",
    "            # Run optimization op (backprop) and cost op (to get loss value)\n",
    "            _, c = sess.run([train_op, loss_op], feed_dict={X: batch_x, Y: batch_y, drop : 1})\n",
    "                                                            \n",
    "            # Compute average loss\n",
    "            avg_cost += c / total_batch\n",
    "            \n",
    "#         tf.summary.scalar('avg_cost',avg_cost)\n",
    "        # Display logs per epoch step\n",
    "#         if epoch % display_step == 0:\n",
    "#             print(\"Epoch:\", '%04d' % (epoch+1), \"cost={:.9f}\".format(avg_cost))\n",
    "\n",
    "    # Test model\n",
    "    pred = tf.nn.softmax(logits)  # Apply softmax to logits\n",
    "    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(Y, 1))\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    print(\"Train Accuracy:\", accuracy.eval({X: data['train'], Y: labels['train'], drop:1 }))\n",
    "    print(\"Test Accuracy:\", accuracy.eval({X: data['val'], Y: labels['val'], drop:1 }))\n",
    "    \n",
    "      # Save the variables to disk.\n",
    "    path = '/Users/josejoy/Desktop/ECE 271B Stat Learning /project/trained models/'\n",
    "    save_path = saver.save(sess, path+\"3layer_MLP.ckpt\")\n",
    "    print(\"Model saved in path: %s\" % save_path)\n",
    "    \n",
    "#     classification = sess.run(pred, feed_dict={X: data['val']})\n",
    "#     print(classification)\n",
    "\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1]), name = 'weights1'),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2]), name = 'weights2'),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes]), name = 'weightsout')\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1]), name = 'bias1'),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2]), name = 'bias2'),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]), name = 'biasout')\n",
    "}\n",
    "\n",
    "\n",
    "# Add ops to save and restore all the variables.\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Later, launch the model, use the saver to restore variables from disk, and\n",
    "# do some work with the model.\n",
    "with tf.Session() as sess:\n",
    "  # Restore variables from disk.\n",
    "    path = '/Users/josejoy/Desktop/ECE 271B Stat Learning /project/trained models/'\n",
    "    saver.restore(sess, path+\"3layer_MLP.ckpt\")\n",
    "    print(\"Model restored.\")\n",
    "  # Check the values of the variables\n",
    "    print(\"h1 : %s\" % weights['h1'].eval())\n",
    "    print(\"b1 : %s\" % biases['b1'].eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
